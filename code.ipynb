{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_base(csv_path):\n",
    "    \"\"\"\n",
    "    Creazione dataframe e preprocessing\n",
    "    Preprocessing: eliminate feature con valori costanti (1)\n",
    "    Preprocessing: cambiate le varie anomalie con nomi diversi in 'anomaly' (2)\n",
    "    Preprocessing: mapping 0: normal 1:anomaly (3)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df=df.loc[:, (df != df.iloc[0]).any()]\n",
    "\n",
    "    df.loc[df['label'] != 'normal', 'label'] = 'anomaly'\n",
    "\n",
    "    df['label'] = df['label'].map({'normal': 0, 'anomaly': 1})\n",
    "\n",
    "    y = df['label']\n",
    "    X = df.drop(columns='label')\n",
    "\n",
    "    return X,y,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_LogisticRegression_pipeline(preprocessor, X_train, y_train):\n",
    "    # LogisticRegression training\n",
    "    modelLog = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))\n",
    "    modelLog.fit(X_train, y_train)\n",
    "    \n",
    "    return modelLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_LDA_pipeline(preprocessor,X_train,y_train):\n",
    "    #LDA training\n",
    "    modelLDA=make_pipeline(preprocessor,LinearDiscriminantAnalysis())\n",
    "    modelLDA.fit(X_train,y_train)\n",
    "    \n",
    "    return modelLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_csv_empty(file_path):\n",
    "    #Utility per scrivere su file csv\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        try:\n",
    "            first_row = next(csvreader)  \n",
    "            return False if first_row else True\n",
    "        except StopIteration:\n",
    "            return True\n",
    "\n",
    "def split_array(array, size): \n",
    "    #Utility per convertire un array in un array bidimensionale composto da array di 5 elementi ciascuno\n",
    "    split_array = []\n",
    "    for i in range(0, len(array), size):\n",
    "        split_array.append(array[i:i+size])\n",
    "    split_array = [subarray for subarray in split_array if len(subarray) == 5]\n",
    "    \n",
    "    return split_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mcc (csv_name,model, X_test, y_test,csv_note=None):\n",
    "    #Calcola MCC e scrive su file csv\n",
    "    MCC=(matthews_corrcoef(y_test, model.predict(X_test))+1)/2 # MCC scalato a [0,1]\n",
    "\n",
    "    if csv_note is not None:\n",
    "        with open(csv_name, 'a', newline='') as file:\n",
    "            fieldnames = ['csv_note','MCC']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            if is_csv_empty(csv_name):\n",
    "                writer.writeheader()\n",
    "\n",
    "            writer.writerow({'csv_note': csv_note,'MCC': MCC})\n",
    "\n",
    "    return MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_cross_val (csv_name,model, X_train, y_train,csv_note=None):\n",
    "    #Calcola accuracy ottenuta tramite cross validation  e scrive su file csv\n",
    "    accuracies_lr = cross_val_score(estimator=model, X=X_train, y=y_train)\n",
    "    avg_accuracy=np.mean(accuracies_lr)\n",
    "\n",
    "    if csv_note is not None:\n",
    "        with open(csv_name, 'a', newline='') as file:\n",
    "            fieldnames = ['csv_note','Accuracy']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            if is_csv_empty(csv_name):\n",
    "                writer.writeheader()\n",
    "\n",
    "            writer.writerow({'csv_note': csv_note,'Accuracy': avg_accuracy})\n",
    "    \n",
    "    return avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_RandomForest_pipeline(preprocessor, X_train, y_train):\n",
    "    # RandomForest training\n",
    "    modelRand = make_pipeline(preprocessor, RandomForestClassifier(random_state=42)) \n",
    "    modelRand.fit(X_train, y_train)\n",
    "    \n",
    "    return modelRand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_XGB_pipeline(preprocessor, X_train, y_train):\n",
    "    # XGB training\n",
    "    modelXGB = make_pipeline(preprocessor, xgb.XGBClassifier(random_state=42)) \n",
    "    modelXGB.fit(X_train, y_train)\n",
    "    \n",
    "    return modelXGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_input_difference(window_length: int, data: pd.DataFrame) -> pd.DataFrame:\n",
    "    #Trasformazione del dataset in TS con differenze \n",
    "    df = data.copy()\n",
    "\n",
    "    i = 1\n",
    "    while i < window_length:\n",
    "        for x in data.columns:\n",
    "            df[f'{x}_{i}'] =data[f'{x}']-data[f'{x}'].shift(i)\n",
    "        i = i + 1\n",
    "\n",
    "    df = df.dropna(axis=0)\n",
    "\n",
    "    for i in range(1, window_length):\n",
    "        df = df.drop(columns=f'label_{i}')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_input_moving_average(window_length: int, data: pd.DataFrame) -> pd.DataFrame:\n",
    "    #Trasformazione del dataset in TS con media mobile \n",
    "    df = data.copy()\n",
    "\n",
    "    for x in data.columns:\n",
    "        df[f'{x}_ma']=df[f'{x}'].rolling(window_length).mean()\n",
    "\n",
    "    df = df.dropna(axis=0)\n",
    "\n",
    "    df = df.drop(columns=f'label_ma')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_detection_score(csv_name,model, X_test: pd.DataFrame, y_test: pd.DataFrame,csv_note=None):\n",
    "    #Calcolo dello speed detection score come media pesata delle frequenze di rilevamento (decrescita quadratica)\n",
    "    X_test['label'] = y_test\n",
    "    X_test=X_test.sort_index()\n",
    "    \n",
    "    X_test_anomalies = X_test[X_test['label'] == 1]\n",
    "    X_test_anomalies = X_test_anomalies.drop(columns='label')\n",
    "\n",
    "    predictions = model.predict(X_test_anomalies)\n",
    "    predictions = split_array(predictions, 5)\n",
    "\n",
    "    counters={}\n",
    "    numerator=0\n",
    "    denominator=0\n",
    "    score=0\n",
    "\n",
    "    for i in range(5):\n",
    "        counters[f'count_{i}']=0\n",
    "\n",
    "    for x in predictions:\n",
    "        if (x[0] == 1):\n",
    "            counters['count_0'] = counters['count_0']+1\n",
    "        elif (x[1] == 1):\n",
    "            counters['count_1'] = counters['count_1']+1\n",
    "        elif (x[2] == 1):\n",
    "            counters['count_2'] = counters['count_2']+1\n",
    "        elif (x[3] == 1):\n",
    "            counters['count_3'] = counters['count_3']+1\n",
    "        elif (x[4] == 1):\n",
    "            counters['count_4'] = counters['count_4']+1\n",
    "\n",
    "    numerator=(counters['count_0']*1+counters['count_1']*0.8**2+counters['count_2']*0.6**2+counters['count_3']*0.4**2+counters['count_4']*0.2**2)\n",
    "    denominator=(counters['count_0']+counters['count_1']+counters['count_2']+counters['count_3']+counters['count_4'])\n",
    "    score=numerator/denominator\n",
    "\n",
    "    if csv_note is not None:\n",
    "        with open(csv_name, 'a', newline='') as file:\n",
    "            fieldnames = ['csv_note','speed_0', 'speed_1','speed_2','speed_3','speed_4']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            if is_csv_empty(csv_name):\n",
    "                writer.writeheader()\n",
    "\n",
    "            writer.writerow({'csv_note': csv_note,'speed_0': counters['count_0'],\n",
    "                             'speed_1':counters['count_1'],'speed_2': counters['count_2'],\n",
    "                             'speed_3':counters['count_3'],'speed_4':counters['count_4']})\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_tree(note, model ,X_train):\n",
    "    #Calcola la feature importance per i modelli basati su alberi decisionali e ne salva il plot delle prime 20 feature\n",
    "    feature_importances = model.steps[-1][1].feature_importances_\n",
    "    features = X_train.columns\n",
    "    importances_df = pd.DataFrame(data={\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "    })\n",
    "    importances_df = importances_df.sort_values(by='Importance', ascending=False).head(20)\n",
    "\n",
    "    plt.bar(x=importances_df['Feature'], height=importances_df['Importance'])\n",
    "    plt.title(f'{note}')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.savefig(f'{note}',bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return importances_df\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_coef(note, model ,X_train):\n",
    "    #Calcola la feature importance per i modelli statistici e ne salva il plot delle prime 20 feature\n",
    "    feature_importances = model.steps[-1][1].coef_[0]\n",
    "    features = X_train.columns\n",
    "    importances_df = pd.DataFrame(data={\n",
    "    'Feature': features,\n",
    "    'Importance': np.abs(feature_importances)\n",
    "    })\n",
    "    importances_df = importances_df.sort_values(by='Importance', ascending=False).head(20)\n",
    "\n",
    "    plt.bar(x=importances_df['Feature'], height=importances_df['Importance'])\n",
    "    plt.title(f'{note}')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.savefig(f'{note}',bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return importances_df\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_classic(shuffle,X,y,fig_details):\n",
    "    #Training dei modelli con approccio classico\n",
    "    preprocessor=StandardScaler()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=shuffle) \n",
    "\n",
    "    modelXGBClassic=my_XGB_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    modelLogClassic=my_LogisticRegression_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    modelRandClassic=my_RandomForest_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    modelLDAClassic=my_LDA_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    classic_models={\n",
    "        'XGB':modelXGBClassic,\n",
    "        'LogisticRegression':modelLogClassic,\n",
    "        'RandomForest':modelRandClassic,\n",
    "        'LDA':modelLDAClassic\n",
    "    }\n",
    "\n",
    "    feature_importance_tree('Feature Importance XGB Classic_'+fig_details,classic_models['XGB'],X_train)\n",
    "    feature_importance_coef('Feature Importance Logistic Regression Classic_'+fig_details,classic_models['LogisticRegression'],X_train)\n",
    "    feature_importance_tree('Feature Importance Random Forest Classic_'+fig_details,classic_models['RandomForest'],X_train)\n",
    "    feature_importance_coef('Feature Importance LDA Classic_'+fig_details,classic_models['LDA'],X_train)\n",
    "    \n",
    "    return classic_models,X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_creation_classic(models,X_train,y_train,X_test,y_test,csv_details):\n",
    "    #Crea dei dizionari che contengono rispettivamente MCC, accuracy e speed score per ogni modello\n",
    "    mcc_map={\n",
    "        'Logistic Regression Classic':get_mcc('mcc_'+csv_details+'.csv',models['LogisticRegression'],X_test,y_test,'Logistic Regression Classic'),\n",
    "        'XGB Classic':get_mcc('mcc_'+csv_details+'.csv',models['XGB'],X_test,y_test,'XGB Classic'),\n",
    "        'Random Forest Classic':get_mcc('mcc_'+csv_details+'.csv',models['RandomForest'],X_test,y_test,'Random Forest Classic'),\n",
    "        'LDA Classic':get_mcc('mcc_'+csv_details+'.csv',models['LDA'],X_test,y_test,'LDA Classic')\n",
    "    }\n",
    "\n",
    "    accuracy_map={\n",
    "        'Logistic Regression Classic':get_accuracy_cross_val('accuracy_'+csv_details+'.csv',models['LogisticRegression'],X_train,y_train,'Logistic Regression Classic'),\n",
    "        'XGB Classic':get_accuracy_cross_val('accuracy_'+csv_details+'.csv',models['XGB'],X_train,y_train,'XGB Classic'),\n",
    "        'Random Forest Classic':get_accuracy_cross_val('accuracy_'+csv_details+'.csv',models['RandomForest'],X_train,y_train,'Random Forest Classic'),\n",
    "        'LDA Classic':get_accuracy_cross_val('accuracy_'+csv_details+'.csv',models['LDA'],X_train,y_train,'LDA Classic')\n",
    "    }\n",
    "\n",
    "    speed_map={\n",
    "        'Logistic Regression Classic':speed_detection_score('speed_'+csv_details+'.csv',models['LogisticRegression'],X_test,y_test,'Logistic Regression Classic'),\n",
    "        'XGB Classic':speed_detection_score('speed_'+csv_details+'.csv',models['XGB'],X_test,y_test,'XGB Classic'),\n",
    "        'Random Forest Classic':speed_detection_score('speed_'+csv_details+'.csv',models['RandomForest'],X_test,y_test,'Random Forest Classic'),\n",
    "        'LDA Classic':speed_detection_score('speed_'+csv_details+'.csv',models['LDA'],X_test,y_test,'LDA Classic')\n",
    "    }\n",
    "    return mcc_map,accuracy_map,speed_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_ts_diff(shuffle,X,y,df,size,fig_details):    \n",
    "    #Training dei modelli con approccio time series con differenze\n",
    "    new_df_difference = window_input_difference(size, df)\n",
    "    y = new_df_difference['label']\n",
    "    X = new_df_difference.drop(columns='label')\n",
    "\n",
    "    preprocessor=StandardScaler()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=shuffle)\n",
    "\n",
    "    modelLogTS=my_LogisticRegression_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    modelRandTS=my_RandomForest_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    modelXGBTS=my_XGB_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    modelLDATS=my_LDA_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    ts_diff_models={\n",
    "        'XGB':modelXGBTS,\n",
    "        'LogisticRegression':modelLogTS,\n",
    "        'RandomForest':modelRandTS,\n",
    "        'LDA':modelLDATS\n",
    "    }\n",
    "\n",
    "    feature_importance_tree('Feature Importance XGB TS Diff_'+fig_details,ts_diff_models['XGB'],X_train)\n",
    "    feature_importance_coef('Feature Importance Logistic Regression TS Diff_'+fig_details,ts_diff_models['LogisticRegression'],X_train)\n",
    "    feature_importance_tree('Feature Importance Random Forest TS Diff_'+fig_details,ts_diff_models['RandomForest'],X_train)\n",
    "    feature_importance_coef('Feature Importance LDA TS Diff_'+fig_details,ts_diff_models['LDA'],X_train)\n",
    "    \n",
    "\n",
    "    return ts_diff_models,X_train,y_train,X_test,y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_update_TS(models,TS_type,X_train,y_train,X_test,y_test,mcc_map,accuracy_map,speed_map,csv_details):\n",
    "    #Update dei dizionari\n",
    "    keys=[None,None,None,None]\n",
    "    if(TS_type=='Diff'):\n",
    "        keys=['Logistic Regression TS Diff','XGB TS Diff','Random Forest TS Diff','LDA TS Diff']\n",
    "    elif (TS_type=='MA'):\n",
    "        keys=['Logistic Regression TS MA','XGB TS MA','Random Forest TS MA','LDA TS MA']\n",
    "\n",
    "    mcc_map.update({\n",
    "        keys[0]:get_mcc('mcc_'+csv_details+'.csv',models['LogisticRegression'],X_test,y_test,keys[0]),\n",
    "        keys[1]:get_mcc('mcc_'+csv_details+'.csv',models['XGB'],X_test,y_test,keys[1]),\n",
    "        keys[2]:get_mcc('mcc_'+csv_details+'.csv',models['RandomForest'],X_test,y_test,keys[2]),\n",
    "        keys[3]:get_mcc('mcc_'+csv_details+'.csv',models['LDA'],X_test,y_test,keys[3])\n",
    "    })\n",
    "\n",
    "    accuracy_map.update({\n",
    "        keys[0]:get_accuracy_cross_val('accuracy_'+csv_details+'.csv',models['LogisticRegression'],X_train,y_train,keys[0]),\n",
    "        keys[1]:get_accuracy_cross_val('accuracy_'+csv_details+'.csv',models['XGB'],X_train,y_train,keys[1]),\n",
    "        keys[2]:get_accuracy_cross_val('accuracy_'+csv_details+'.csv',models['RandomForest'],X_train,y_train,keys[2]),\n",
    "        keys[3]:get_accuracy_cross_val('accuracy_'+csv_details+'.csv',models['LDA'],X_train,y_train,keys[3])\n",
    "    })\n",
    "\n",
    "    speed_map.update({\n",
    "        keys[0]:speed_detection_score('speed_'+csv_details+'.csv',models['LogisticRegression'],X_test,y_test,keys[0]),\n",
    "        keys[1]:speed_detection_score('speed_'+csv_details+'.csv',models['XGB'],X_test,y_test,keys[1]),\n",
    "        keys[2]:speed_detection_score('speed_'+csv_details+'.csv',models['RandomForest'],X_test,y_test,keys[2]),\n",
    "        keys[3]:speed_detection_score('speed_'+csv_details+'.csv',models['LDA'],X_test,y_test,keys[3])\n",
    "    })\n",
    "    return mcc_map,accuracy_map,speed_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_ts_ma(shuffle,X,y,df,size,fig_details):\n",
    "    #Training dei modelli con approccio time series con media mobile\n",
    "    new_df_moving_average = window_input_moving_average(size, df)\n",
    "    y = new_df_moving_average['label']\n",
    "    X = new_df_moving_average.drop(columns='label')\n",
    "\n",
    "    preprocessor=StandardScaler()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=shuffle)\n",
    "\n",
    "    modelLogTSMA=my_LogisticRegression_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    modelRandTSMA=my_RandomForest_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    modelXGBTSMA=my_XGB_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    modelLDATSMA=my_LDA_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    ts_ma_models={\n",
    "        'XGB':modelXGBTSMA,\n",
    "        'LogisticRegression':modelLogTSMA,\n",
    "        'RandomForest':modelRandTSMA,\n",
    "        'LDA':modelLDATSMA\n",
    "    }\n",
    "\n",
    "    feature_importance_tree('Feature Importance XGB TS MA_'+fig_details,ts_ma_models['XGB'],X_train)\n",
    "    feature_importance_coef('Feature Importance Logistic Regression TS MA_'+fig_details,ts_ma_models['LogisticRegression'],X_train)\n",
    "    feature_importance_tree('Feature Importance Random Forest TS MA_'+fig_details,ts_ma_models['RandomForest'],X_train)\n",
    "    feature_importance_coef('Feature Importance LDA TS MA_'+fig_details,ts_ma_models['LDA'],X_train)\n",
    "\n",
    "    return ts_ma_models,X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progression_plot(ax,map,note,mcc_map=None):\n",
    "    #Utility per costruire i plot di progressione per MCC,error rate,accuracy e speed score\n",
    "    x_values = list(map.keys())\n",
    "    map_values=list(map.values()) #Pu√≤ essere speed_map o accuracy_map\n",
    "\n",
    "    if mcc_map==None:\n",
    "        ax.plot(x_values, map_values, marker='o', linestyle='-', label='Speed Score')\n",
    "    else:\n",
    "        err_values=[1-x for x in map_values]\n",
    "        mcc_values = list(mcc_map.values())\n",
    "        ax.bar(x_values, err_values,color='orange')\n",
    "        ax.set_ylabel('Error Rate', color='orange')  \n",
    "        ax2=ax.twinx()\n",
    "        ax2.plot(x_values, mcc_values, marker='o', linestyle='-')\n",
    "        ax2.set_ylabel('MCC', color='blue')  \n",
    "        ax.legend()\n",
    "        ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc_accuracy_progression(mcc_map,accuracy_map,details):\n",
    "    #Plot progressione MCC, error rate e accuracy \n",
    "    logistic_mcc_map={}\n",
    "    random_forest_mcc_map={}\n",
    "    xgb_mcc_map={}\n",
    "    lda_mcc_map={}\n",
    "\n",
    "    logistic_accuracy_map={}\n",
    "    random_forest_accuracy_map={}\n",
    "    xgb_accuracy_map={}\n",
    "    lda_accuracy_map={}\n",
    "\n",
    "    for key, value in mcc_map.items():\n",
    "        if 'Logistic' in key:\n",
    "            logistic_mcc_map[key] = value\n",
    "        if 'Random' in key:\n",
    "            random_forest_mcc_map[key] = value\n",
    "        if 'XGB' in key:\n",
    "            xgb_mcc_map[key] = value\n",
    "        if 'LDA' in key:\n",
    "            lda_mcc_map[key] = value\n",
    "\n",
    "    for key, value in accuracy_map.items():\n",
    "        if 'Logistic' in key:\n",
    "            logistic_accuracy_map[key] = value\n",
    "        if 'Random' in key:\n",
    "            random_forest_accuracy_map[key] = value\n",
    "        if 'XGB' in key:\n",
    "            xgb_accuracy_map[key] = value\n",
    "        if 'LDA' in key:\n",
    "            lda_accuracy_map[key] = value\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(18,12))\n",
    "    fig.suptitle('MCC Progression & Error Rate Progression '+details)\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    fig.subplots_adjust(hspace=0.3)\n",
    "    progression_plot(ax[0,0],logistic_accuracy_map,'Logistic_Regression_MCC_Progression',logistic_mcc_map)\n",
    "    progression_plot(ax[0,1],xgb_accuracy_map,'XGB_MCC_Progression',xgb_mcc_map)\n",
    "    progression_plot(ax[1,0],random_forest_accuracy_map,'Random_Forest_MCC_Progression',random_forest_mcc_map)\n",
    "    progression_plot(ax[1,1],lda_accuracy_map,'LDA_MCC_Progression',lda_mcc_map)\n",
    "    plt.savefig('MCC_Progression_'+details)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(18,12))\n",
    "    fig.suptitle('Accuracy Progression '+details)\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    fig.subplots_adjust(hspace=0.3)\n",
    "    progression_plot(ax[0,0],logistic_accuracy_map,'Logistic_Regression_Accuracy_Progression')\n",
    "    progression_plot(ax[0,1],xgb_accuracy_map,'XGB_Accuracy_Progression')\n",
    "    progression_plot(ax[1,0],random_forest_accuracy_map,'Random_Forest_Accuracy_Progression')\n",
    "    progression_plot(ax[1,1],lda_accuracy_map,'LDA_Accuracy_Progression')\n",
    "    plt.savefig('Accuracy_Progression_'+details)\n",
    "\n",
    "    return logistic_mcc_map,random_forest_mcc_map,xgb_mcc_map,lda_mcc_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_progression(speed_map,details):\n",
    "    #Plot progressione speed score \n",
    "    logistic_speed_map={}\n",
    "    random_forest_speed_map={}\n",
    "    xgb_speed_map={}\n",
    "    lda_speed_map={}\n",
    "\n",
    "    for key, value in speed_map.items():\n",
    "        if 'Logistic' in key:\n",
    "            logistic_speed_map[key] = value\n",
    "        if 'Random' in key:\n",
    "            random_forest_speed_map[key] = value\n",
    "        if 'XGB' in key:\n",
    "            xgb_speed_map[key] = value\n",
    "        if 'LDA' in key:\n",
    "            lda_speed_map[key] = value\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(18,12))\n",
    "    fig.suptitle('Detection Speed Progression '+details)\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    fig.subplots_adjust(hspace=0.3)\n",
    "    progression_plot(ax[0,0],logistic_speed_map,'Logistic_Regression_Speed_Progression')\n",
    "    progression_plot(ax[0,1],xgb_speed_map,'XGB_Speed_Progression')\n",
    "    progression_plot(ax[1,0],random_forest_speed_map,'Random_Forest_Speed_Progression')\n",
    "    progression_plot(ax[1,1],lda_speed_map,'LDA_Speed_Progression')\n",
    "    plt.savefig('Speed_Progression_'+details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc_all3(classic_models,ts_diff_models,ts_ma_models,size,shuffle):\n",
    "    #Calcolo MCC per tutti i modelli usando come test set il dataset my_all3\n",
    "    homenet_df=pd.read_csv('preprocessed_arancino_datasets/homenet_filtered.csv')\n",
    "    mobile_df=pd.read_csv('preprocessed_arancino_datasets/mobile_filtered.csv')\n",
    "    unifi_df=pd.read_csv('preprocessed_arancino_datasets/unifi_filtered.csv')\n",
    "    y=unifi_df['label']\n",
    "    X=unifi_df.drop(columns='label')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=shuffle)\n",
    "    my_unifi_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    my_all3_df = pd.concat([homenet_df,mobile_df,my_unifi_df],axis=0)\n",
    "    my_all3_df.to_csv(\"preprocessed_arancino_datasets/my_all3.csv\", index=False)\n",
    "\n",
    "    X,y,df=preprocessing_base('preprocessed_arancino_datasets/my_all3.csv')\n",
    "\n",
    "    mcc_all_map={\n",
    "        'Logistic Regression Classic':get_mcc('',classic_models['LogisticRegression'],X,y),\n",
    "        'XGB Classic':get_mcc('',classic_models['XGB'],X,y),\n",
    "        'Random Forest Classic':get_mcc('',classic_models['RandomForest'],X,y),\n",
    "        'LDA Classic':get_mcc('',classic_models['LDA'],X,y)\n",
    "    }\n",
    "    \n",
    "    new_df_moving_average=window_input_moving_average(size,df)\n",
    "    y = new_df_moving_average['label']\n",
    "    X = new_df_moving_average.drop(columns='label')\n",
    "\n",
    "    mcc_all_map.update({\n",
    "        'Logistic Regression TS MA':get_mcc('',ts_ma_models['LogisticRegression'],X,y),\n",
    "        'XGB TS MA':get_mcc('',ts_ma_models['XGB'],X,y),\n",
    "        'Random Forest TS MA':get_mcc('',ts_ma_models['RandomForest'],X,y),\n",
    "        'LDA TS MA':get_mcc('',ts_ma_models['LDA'],X,y)\n",
    "    })\n",
    "\n",
    "    new_df_difference = window_input_difference(size, df)\n",
    "    y = new_df_difference['label']\n",
    "    X = new_df_difference.drop(columns='label')\n",
    "\n",
    "    mcc_all_map.update({\n",
    "        'Logistic Regression TS Diff':get_mcc('',ts_diff_models['LogisticRegression'],X,y),\n",
    "        'XGB TS Diff':get_mcc('',ts_diff_models['XGB'],X,y),\n",
    "        'Random Forest TS Diff':get_mcc('',ts_diff_models['RandomForest'],X,y),\n",
    "        'LDA TS Diff':get_mcc('',ts_diff_models['LDA'],X,y)\n",
    "    })\n",
    "    \n",
    "    return mcc_all_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc_all3_plot(mcc_all_map,logistic_mcc_map,random_forest_mcc_map,xgb_mcc_map,lda_mcc_map,details):    \n",
    "    #Plot per comparare MCC sul test set di Uni con quello su my_all3\n",
    "    logistic_mcc_all_map={}\n",
    "    random_forest_mcc_all_map={}\n",
    "    xgb_mcc_all_map={}\n",
    "    lda_mcc_all_map={}\n",
    "\n",
    "    #Split maps\n",
    "    for key, value in mcc_all_map.items():\n",
    "        if 'Logistic' in key:\n",
    "            logistic_mcc_all_map[key] = value\n",
    "        if 'Random' in key:\n",
    "            random_forest_mcc_all_map[key] = value\n",
    "        if 'XGB' in key:\n",
    "            xgb_mcc_all_map[key] = value\n",
    "        if 'LDA' in key:\n",
    "            lda_mcc_all_map[key] = value\n",
    "\n",
    "    x_values = list(logistic_mcc_all_map.keys())\n",
    "    y_values=list(logistic_mcc_map.values())\n",
    "    y_all_values = list(logistic_mcc_all_map.values())\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    ax[0,0].plot(x_values, y_values, marker='o', linestyle='-',label='MCC unifi')\n",
    "    ax[0,0].plot(x_values, y_all_values, marker='o', linestyle='-',label='MCC my_all3')\n",
    "    ax[0,0].legend()\n",
    "\n",
    "    x_values = list(xgb_mcc_map.keys())\n",
    "    y_values=list(xgb_mcc_map.values())\n",
    "    y_all_values = list(xgb_mcc_all_map.values())\n",
    "\n",
    "    ax[0,1].plot(x_values, y_values, marker='o', linestyle='-',label='MCC unifi')\n",
    "    ax[0,1].plot(x_values, y_all_values, marker='o', linestyle='-',label='MCC my_all3')\n",
    "    ax[0,1].legend()\n",
    "\n",
    "    x_values = list(random_forest_mcc_map.keys())\n",
    "    y_values=list(random_forest_mcc_map.values())\n",
    "    y_all_values = list(random_forest_mcc_all_map.values())\n",
    "\n",
    "    ax[1,0].plot(x_values, y_values, marker='o', linestyle='-',label='MCC unifi')\n",
    "    ax[1,0].plot(x_values, y_all_values, marker='o', linestyle='-',label='MCC my_all3')\n",
    "    ax[1,0].legend()\n",
    "\n",
    "    x_values = list(lda_mcc_map.keys())\n",
    "    y_values=list(lda_mcc_map.values())\n",
    "    y_all_values = list(lda_mcc_all_map.values())\n",
    "\n",
    "    ax[1,1].plot(x_values, y_values, marker='o', linestyle='-',label='MCC unifi')\n",
    "    ax[1,1].plot(x_values, y_all_values, marker='o', linestyle='-',label='MCC my_all3')\n",
    "    ax[1,1].legend()\n",
    "\n",
    "    fig.suptitle('MCC Comparison unifi-my_all3 '+details)\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    fig.subplots_adjust(hspace=0.3)\n",
    "    plt.savefig('MCC Comparison Uni-All3 '+details)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_win5_shufTrue():\n",
    "    X,y,df=preprocessing_base('preprocessed_arancino_datasets/unifi_filtered.csv')\n",
    "\n",
    "    shuffle=True\n",
    "    size=5\n",
    "    details='win5_shufTrue'\n",
    "    \n",
    "    classic_models,X_train,y_train,X_test,y_test=training_classic(shuffle,X,y,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_creation_classic(classic_models,X_train,y_train,X_test,y_test,details)\n",
    "\n",
    "    ts_ma_models,X_train,y_train,X_test,y_test=training_ts_ma(shuffle,X,y,df,size,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_update_TS(ts_ma_models,'MA',X_train,y_train,X_test,y_test,mcc_map,accuracy_map,speed_map,details)\n",
    "\n",
    "    ts_diff_models,X_train,y_train,X_test,y_test=training_ts_diff(shuffle,X,y,df,size,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_update_TS(ts_diff_models,'Diff',X_train,y_train,X_test,y_test,mcc_map,accuracy_map,speed_map,details)\n",
    "\n",
    "    logistic_mcc_map,random_forest_mcc_map,xgb_mcc_map,lda_mcc_map=mcc_accuracy_progression(mcc_map,accuracy_map,details)\n",
    "    mcc_all_map=mcc_all3(classic_models,ts_diff_models,ts_ma_models,size,shuffle)\n",
    "    mcc_all3_plot(mcc_all_map,logistic_mcc_map,random_forest_mcc_map,xgb_mcc_map,lda_mcc_map,details)\n",
    "    speed_progression(speed_map,details)\n",
    "\n",
    "    return mcc_all_map\n",
    "\n",
    "mcc_all_map_main=main_win5_shufTrue()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_win5_shufFalse():\n",
    "    X,y,df=preprocessing_base('preprocessed_arancino_datasets/unifi_filtered.csv')\n",
    "\n",
    "    shuffle=False\n",
    "    size=5\n",
    "    details='win5_shufFalse'\n",
    "    \n",
    "    classic_models,X_train,y_train,X_test,y_test=training_classic(shuffle,X,y,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_creation_classic(classic_models,X_train,y_train,X_test,y_test,details)\n",
    "\n",
    "    ts_ma_models,X_train,y_train,X_test,y_test=training_ts_ma(shuffle,X,y,df,size,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_update_TS(ts_ma_models,'MA',X_train,y_train,X_test,y_test,mcc_map,accuracy_map,speed_map,details)\n",
    "\n",
    "    ts_diff_models,X_train,y_train,X_test,y_test=training_ts_diff(shuffle,X,y,df,size,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_update_TS(ts_diff_models,'Diff',X_train,y_train,X_test,y_test,mcc_map,accuracy_map,speed_map,details)\n",
    "\n",
    "    logistic_mcc_map,random_forest_mcc_map,xgb_mcc_map,lda_mcc_map=mcc_accuracy_progression(mcc_map,accuracy_map,details)\n",
    "    mcc_all_map=mcc_all3(classic_models,ts_diff_models,ts_ma_models,size,shuffle)\n",
    "    mcc_all3_plot(mcc_all_map,logistic_mcc_map,random_forest_mcc_map,xgb_mcc_map,lda_mcc_map,details)\n",
    "    speed_progression(speed_map,details)\n",
    "\n",
    "    return xgb_mcc_map,logistic_mcc_map,random_forest_mcc_map,lda_mcc_map\n",
    "\n",
    "xgb_mcc_map_win5,logistic_mcc_map_win5,random_forest_mcc_map_win5,lda_mcc_map_win5=main_win5_shufFalse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_win4_shufFalse():\n",
    "    X,y,df=preprocessing_base('preprocessed_arancino_datasets/unifi_filtered.csv')\n",
    "\n",
    "    shuffle=False\n",
    "    size=4\n",
    "    details='win4_shufFalse'\n",
    "    \n",
    "    classic_models,X_train,y_train,X_test,y_test=training_classic(shuffle,X,y,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_creation_classic(classic_models,X_train,y_train,X_test,y_test,details)\n",
    "\n",
    "    ts_ma_models,X_train,y_train,X_test,y_test=training_ts_ma(shuffle,X,y,df,size,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_update_TS(ts_ma_models,'MA',X_train,y_train,X_test,y_test,mcc_map,accuracy_map,speed_map,details)\n",
    "\n",
    "    ts_diff_models,X_train,y_train,X_test,y_test=training_ts_diff(shuffle,X,y,df,size,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_update_TS(ts_diff_models,'Diff',X_train,y_train,X_test,y_test,mcc_map,accuracy_map,speed_map,details)\n",
    "\n",
    "    logistic_mcc_map,random_forest_mcc_map,xgb_mcc_map,lda_mcc_map=mcc_accuracy_progression(mcc_map,accuracy_map,details)\n",
    "    mcc_all_map=mcc_all3(classic_models,ts_diff_models,ts_ma_models,size,shuffle)\n",
    "    mcc_all3_plot(mcc_all_map,logistic_mcc_map,random_forest_mcc_map,xgb_mcc_map,lda_mcc_map,details)\n",
    "    speed_progression(speed_map,details)\n",
    "\n",
    "    return xgb_mcc_map,logistic_mcc_map,random_forest_mcc_map,lda_mcc_map\n",
    "\n",
    "xgb_mcc_map_win4,logistic_mcc_map_win4,random_forest_mcc_map_win4,lda_mcc_map_win4=main_win4_shufFalse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_win3_shufFalse():\n",
    "    X,y,df=preprocessing_base('preprocessed_arancino_datasets/unifi_filtered.csv')\n",
    "\n",
    "    shuffle=False\n",
    "    size=3\n",
    "    details='win3_shufFalse'\n",
    "    \n",
    "    classic_models,X_train,y_train,X_test,y_test=training_classic(shuffle,X,y,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_creation_classic(classic_models,X_train,y_train,X_test,y_test,details)\n",
    "\n",
    "    ts_ma_models,X_train,y_train,X_test,y_test=training_ts_ma(shuffle,X,y,df,size,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_update_TS(ts_ma_models,'MA',X_train,y_train,X_test,y_test,mcc_map,accuracy_map,speed_map,details)\n",
    "\n",
    "    ts_diff_models,X_train,y_train,X_test,y_test=training_ts_diff(shuffle,X,y,df,size,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_update_TS(ts_diff_models,'Diff',X_train,y_train,X_test,y_test,mcc_map,accuracy_map,speed_map,details)\n",
    "\n",
    "    logistic_mcc_map,random_forest_mcc_map,xgb_mcc_map,lda_mcc_map=mcc_accuracy_progression(mcc_map,accuracy_map,details)\n",
    "    mcc_all_map=mcc_all3(classic_models,ts_diff_models,ts_ma_models,size,shuffle)\n",
    "    mcc_all3_plot(mcc_all_map,logistic_mcc_map,random_forest_mcc_map,xgb_mcc_map,lda_mcc_map,details)\n",
    "    speed_progression(speed_map,details)\n",
    "\n",
    "    return xgb_mcc_map,logistic_mcc_map,random_forest_mcc_map,lda_mcc_map\n",
    "\n",
    "xgb_mcc_map_win3,logistic_mcc_map_win3,random_forest_mcc_map_win3,lda_mcc_map_win3=main_win3_shufFalse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_win2_shufFalse():\n",
    "    X,y,df=preprocessing_base('preprocessed_arancino_datasets/unifi_filtered.csv')\n",
    "\n",
    "    shuffle=False\n",
    "    size=2\n",
    "    details='win2_shufFalse'\n",
    "    \n",
    "    classic_models,X_train,y_train,X_test,y_test=training_classic(shuffle,X,y,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_creation_classic(classic_models,X_train,y_train,X_test,y_test,details)\n",
    "\n",
    "    ts_ma_models,X_train,y_train,X_test,y_test=training_ts_ma(shuffle,X,y,df,size,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_update_TS(ts_ma_models,'MA',X_train,y_train,X_test,y_test,mcc_map,accuracy_map,speed_map,details)\n",
    "\n",
    "    ts_diff_models,X_train,y_train,X_test,y_test=training_ts_diff(shuffle,X,y,df,size,details)\n",
    "    mcc_map,accuracy_map,speed_map=map_update_TS(ts_diff_models,'Diff',X_train,y_train,X_test,y_test,mcc_map,accuracy_map,speed_map,details)\n",
    "\n",
    "    logistic_mcc_map,random_forest_mcc_map,xgb_mcc_map,lda_mcc_map=mcc_accuracy_progression(mcc_map,accuracy_map,details)\n",
    "    mcc_all_map=mcc_all3(classic_models,ts_diff_models,ts_ma_models,size,shuffle)\n",
    "    mcc_all3_plot(mcc_all_map,logistic_mcc_map,random_forest_mcc_map,xgb_mcc_map,lda_mcc_map,details)\n",
    "    speed_progression(speed_map,details)\n",
    "\n",
    "    return xgb_mcc_map,logistic_mcc_map,random_forest_mcc_map,lda_mcc_map\n",
    "\n",
    "xgb_mcc_map_win2,logistic_mcc_map_win2,random_forest_mcc_map_win2,lda_mcc_map_win2=main_win2_shufFalse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_unreliable_features_classic(columns_to_drop,model_name,csv_name):  \n",
    "    #Elimina le feature 'inaffidabili' dai modelli con shuffle=False\n",
    "    X,y,df=preprocessing_base('preprocessed_arancino_datasets/unifi_filtered.csv')\n",
    "    \n",
    "    X=X.drop(columns=columns_to_drop)\n",
    "    df=df.drop(columns=columns_to_drop)\n",
    "\n",
    "    preprocessor=StandardScaler()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False) \n",
    "\n",
    "    if(model_name=='XGB Classic'):\n",
    "        model=my_XGB_pipeline(preprocessor,X_train,y_train)\n",
    "    elif(model_name=='Logistic Regression Classic'):\n",
    "        model=my_LogisticRegression_pipeline(preprocessor,X_train,y_train)\n",
    "    elif(model_name=='Random Forest Classic'):\n",
    "        model=my_RandomForest_pipeline(preprocessor,X_train,y_train)\n",
    "    elif(model_name=='LDA Classic'):\n",
    "        model=my_LDA_pipeline(preprocessor,X_train,y_train)\n",
    "\n",
    "    MCC=(matthews_corrcoef(y_test, model.predict(X_test))+1)/2 # MCC scalato a [0,1]\n",
    "\n",
    "    with open('mcc_csv/mcc_win5_shufFalse.csv', 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            if row[0]==model_name:\n",
    "                value = row[1]\n",
    "\n",
    "    with open(csv_name, 'a', newline='') as file:\n",
    "            fieldnames = ['csv_note','MCC Without Unreliable Features','MCC With Unreliable Features']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            if is_csv_empty(csv_name):\n",
    "                writer.writeheader()\n",
    "\n",
    "            writer.writerow({'csv_note': model_name,'MCC Without Unreliable Features': MCC,'MCC With Unreliable Features': value})\n",
    "\n",
    "    return model,X,y\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def mcc_plot_all3_unreliable(mcc_all_map_main1, mcc_map_without_unreliable_features):\n",
    "    #Plot per confrontare MCC dei modelli addestrati con e senza feature inaffidabili su my_all3  \n",
    "    x_values = list(itertools.islice(mcc_all_map_main1.keys(), 4))\n",
    "    mcc_map_with_unreliable_features_values=list(itertools.islice(mcc_all_map_main1.values(), 4))\n",
    "    mcc_map_without_unreliable_features_values=list(mcc_map_without_unreliable_features.values())\n",
    "\n",
    "    plt.suptitle('MCC Comparison With/Without Unreliable Features on my_all3 ')\n",
    "    plt.plot(x_values, mcc_map_with_unreliable_features_values, marker='o', linestyle='-',label='With Unreliable Features')\n",
    "    plt.plot(x_values, mcc_map_without_unreliable_features_values, marker='o', linestyle='-',label='Without Unreliable Features')\n",
    "    plt.legend()\n",
    "    plt.savefig('MCC_Comparison_With_Without_Unreliable_Features_All3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimina Logistic Regression Unreliable Features\n",
    "columns_to_drop=['meminfo.MemFree','iostat.nice','disk_io.read_time','redis_used_cpu_sys','cpu_times.idle']\n",
    "model_name='Logistic Regression Classic'\n",
    "csv_name='mcc_without_unreliable_features.csv'\n",
    "\n",
    "X,y,df=preprocessing_base('preprocessed_arancino_datasets/my_all3.csv')\n",
    "\n",
    "model,X,y=delete_unreliable_features_classic(columns_to_drop,model_name,csv_name)\n",
    "MCC=(matthews_corrcoef(y, model.predict(X))+1)/2 # MCC scalato a [0,1]\n",
    "\n",
    "mcc_unreliable_map={\n",
    "    model_name:MCC\n",
    "}\n",
    "\n",
    "#Elimina XGB Unreliable Features\n",
    "columns_to_drop=['net_io.bytes_sent','meminfo.Inactive','disk_io.read_bytes','cpu_times.iowait','vmstat.pgfree','temperature.temperature']\n",
    "model_name='XGB Classic'\n",
    "\n",
    "X,y,df=preprocessing_base('preprocessed_arancino_datasets/my_all3.csv')\n",
    "\n",
    "model,X,y=delete_unreliable_features_classic(columns_to_drop,model_name,csv_name)\n",
    "MCC=(matthews_corrcoef(y, model.predict(X))+1)/2 # MCC scalato a [0,1]\n",
    "\n",
    "mcc_unreliable_map.update(\n",
    "    {model_name:MCC}\n",
    ")\n",
    "\n",
    "#Elimina Random Forest Unreliable Features\n",
    "columns_to_drop=['meminfo.MemAvailable','virtual.available','meminfo.Inactive','virtual.inactive']\n",
    "model_name='Random Forest Classic'\n",
    "\n",
    "X,y,df=preprocessing_base('preprocessed_arancino_datasets/my_all3.csv')\n",
    "\n",
    "model,X,y=delete_unreliable_features_classic(columns_to_drop,model_name,csv_name)\n",
    "MCC=(matthews_corrcoef(y, model.predict(X))+1)/2 # MCC scalato a [0,1]\n",
    "\n",
    "mcc_unreliable_map.update(\n",
    "    {model_name:MCC}\n",
    ")\n",
    "\n",
    "#Elimina LDA Unreliable Features\n",
    "columns_to_drop=['cpu_times.idle','cpu_stats.ctx_switches','vmstat.pgreuse','cpu_times.nice','netinfo.lo.sent.pkts']\n",
    "model_name='LDA Classic'\n",
    "\n",
    "X,y,df=preprocessing_base('preprocessed_arancino_datasets/my_all3.csv')\n",
    "\n",
    "model,X,y=delete_unreliable_features_classic(columns_to_drop,model_name,csv_name)\n",
    "MCC=(matthews_corrcoef(y, model.predict(X))+1)/2 # MCC scalato a [0,1]\n",
    "\n",
    "mcc_unreliable_map.update(\n",
    "    {model_name:MCC}\n",
    ")\n",
    "\n",
    "mcc_plot_all3_unreliable(mcc_all_map_main,mcc_unreliable_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mcc_progression_window(mcc_win5_map,mcc_win4_map,mcc_win3_map,mcc_win2_map,details):\n",
    "    #Plot per confrontare progresione MCC dei modelli al variare della finestra temporale utilizzata  \n",
    "    x_values = ['win5','win4','win3','win2']\n",
    "    mcc_values_ma=[mcc_win5_map[details+' TS MA'],mcc_win4_map[details+' TS MA'],mcc_win3_map[details+' TS MA'],mcc_win2_map[details+' TS MA']]\n",
    "    mcc_values_diff=[mcc_win5_map[details+' TS Diff'],mcc_win4_map[details+' TS Diff'],mcc_win3_map[details+' TS Diff'],mcc_win2_map[details+' TS Diff']]\n",
    "\n",
    "    if(details=='XGB'):\n",
    "        details='XGBoost'\n",
    "\n",
    "    plt.suptitle('MCC Progression by Window Size '+details)\n",
    "    plt.plot(x_values, mcc_values_ma, marker='o', linestyle='-',label='TS MA')\n",
    "    plt.plot(x_values, mcc_values_diff, marker='o', linestyle='-',label='TS Diff')\n",
    "    plt.legend()\n",
    "    plt.savefig('MCC_Progression_by_Window_Size_'+details)\n",
    "    plt.clf()\n",
    "\n",
    "mcc_progression_window(xgb_mcc_map_win5,xgb_mcc_map_win4,xgb_mcc_map_win3,xgb_mcc_map_win2,'XGB')\n",
    "mcc_progression_window(logistic_mcc_map_win5,logistic_mcc_map_win4,logistic_mcc_map_win3,logistic_mcc_map_win2,'Logistic Regression')\n",
    "mcc_progression_window(random_forest_mcc_map_win5,random_forest_mcc_map_win4,random_forest_mcc_map_win3,random_forest_mcc_map_win2,'Random Forest')\n",
    "mcc_progression_window(lda_mcc_map_win5,lda_mcc_map_win4,lda_mcc_map_win3,lda_mcc_map_win2,'LDA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
